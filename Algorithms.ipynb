{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gridworld import GridworldEnv\n",
    "from Algorithms.single_q_learning import SingleQLearning\n",
    "from Algorithms.double_q_learning import DoubleQLearning\n",
    "from Algorithms.single_dqn import SingleDQN\n",
    "from Algorithms.double_dqn import DoubleDQN\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import random\n",
    "import torch\n",
    "import gym\n",
    "\n",
    "from customgame import FirstGame\n",
    "\n",
    "import time\n",
    "assert sys.version_info[:3] >= (3, 6, 0), \"Make sure you have Python 3.6 installed!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridworldEnv()\n",
    "\n",
    "# env = FirstGame()\n",
    "\n",
    "def running_mean(vals, n=1):\n",
    "    cumvals = np.array(vals).cumsum()\n",
    "    return (cumvals[n:] - cumvals[:-n]) / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_a = np.zeros((env.nS, env.nA))\n",
    "Q_b = np.zeros((env.nS, env.nA))\n",
    "policy = DoubleQLearning.EpsilonGreedyPolicy(Q_a, Q_b, epsilon=0.1)\n",
    "Q_q_learning, (episode_lengths_double_q_learning, episode_returns_double_q_learning) = DoubleQLearning.double_q_learning(env, policy, Q_a, Q_b, 1000)\n",
    "\n",
    "Q = np.zeros((env.nS, env.nA))\n",
    "policy = SingleQLearning.EpsilonGreedyPolicy(Q, epsilon=0.1)\n",
    "Q_q_learning, (episode_lengths_single_q_learning, episode_returns_single_q_learning) = SingleQLearning.single_q_learning(env, policy, Q, 1000)\n",
    "\n",
    "n = 50\n",
    "\n",
    "plt.plot(running_mean(episode_lengths_double_q_learning, n))\n",
    "plt.plot(running_mean(episode_lengths_single_q_learning, n))\n",
    "plt.title('Episode lengths Q-learning')\n",
    "plt.show()\n",
    "plt.plot(running_mean(episode_returns_double_q_learning,n))\n",
    "plt.plot(running_mean(episode_returns_single_q_learning,n))\n",
    "plt.title('Episode returns Q-learning')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.envs.make(\"CartPole-v1\")\n",
    "SingleDQN = SingleDQN()\n",
    "\n",
    "num_episodes = 200\n",
    "batch_size = 64\n",
    "discount_factor = 0.8\n",
    "learn_rate = 1e-3\n",
    "memory = SingleDQN.ReplayMemory(10000)\n",
    "num_hidden = 128\n",
    "seed = 42  # This is not randomly chosen\n",
    "\n",
    "# We will seed the algorithm (before initializing QNetwork!) for reproducibility\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "env.seed(seed)\n",
    "\n",
    "Q_net = SingleDQN.QNetwork(num_hidden)\n",
    "policy = SingleDQN.EpsilonGreedyPolicy(Q_net, 0.05)\n",
    "episode_durations = SingleDQN.run_episodes(Q_net, policy, memory, env, num_episodes, batch_size, discount_factor, learn_rate)\n",
    "\n",
    "plt.plot(running_mean(episode_durations, 10))\n",
    "plt.title('Episode durations per episode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/david/anaconda3/envs/rlcourse/lib/python3.7/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Q_net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/_f/bfqr_zfd6_vfr5ty9w7mlwfm0000gn/T/ipykernel_34560/4004362540.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mQ_net_online\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDoubleDQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mQ_net_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDoubleDQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDoubleDQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEpsilonGreedyPolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mepisode_durations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDoubleDQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_episodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ_net_online\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ_net_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscount_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Q_net' is not defined"
     ]
    }
   ],
   "source": [
    "env = gym.envs.make(\"CartPole-v1\")\n",
    "DoubleDQN = DoubleDQN()\n",
    "\n",
    "num_episodes = 200\n",
    "batch_size = 64\n",
    "discount_factor = 0.8\n",
    "learn_rate = 1e-3\n",
    "memory = SingleDQN.ReplayMemory(10000)\n",
    "num_hidden = 128\n",
    "seed = 42  # This is not randomly chosen\n",
    "\n",
    "# We will seed the algorithm (before initializing QNetwork!) for reproducibility\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "env.seed(seed)\n",
    "\n",
    "Q_net_online = DoubleDQN.QNetwork(num_hidden)\n",
    "Q_net_target = DoubleDQN.QNetwork(num_hidden)\n",
    "policy = DoubleDQN.EpsilonGreedyPolicy(Q_net_online, 0.05)\n",
    "episode_durations = DoubleDQN.run_episodes(Q_net_online, Q_net_target, policy, memory, env, num_episodes, batch_size, discount_factor, learn_rate)\n",
    "\n",
    "plt.plot(running_mean(episode_durations, 10))\n",
    "plt.title('Episode durations per episode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c2e7c519efe6d01b138faa356f66a9cb564bc42de5493895111f35cdc0f09716"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('rlcourse': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
